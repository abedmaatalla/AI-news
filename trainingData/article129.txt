 the spread of isis propaganda online has put social media companies in a tough position . governments are urging facebook , twitter , and google to more aggressively remove extremist content , in the hopes of reducing the terrorist group s influence . but the companies self moderation systems have struggled to keep pace , and terrorist material continues to spread online . now , a nonprofit organization has developed an algorithm that it says can automate the removal of terrorist related content . but there are concerns that it could infringe on freedom of speech , and some question whether automated content removal would mitigate radicalization . the algorithm , called eglyph , was announced in june by the counter extremism project cep , a new york based nonprofit organization that tracks extremist groups . eglyph uses so called hashing technology to assign a unique fingerprint to images , videos , and audio that have already been flagged as extremist , and automatically removes any versions that have been uploaded to a social network . it will also automatically delete other versions as soon as users attempt to upload them . a game changer for social media companies ? eglyph was developed by hany farid , professor of computer science at dartmouth college , and is modeled on the photodna algorithm that farid created with microsoft in to combat child pornography . both photodna and eglyph are capable of recognizing images even if they ve been resized or altered , but eglyph extends the technology to video and audio . youtube s contentid system uses similar technology to identify copyright infringing videos . eglyph is in its final stages of testing , and will be made freely available to social media companies . the cep is also compiling a database of extremist content , called the national office on reporting extremism norex , which it hopes will become a comprehensive resource for researchers and social media companies . mark wallace , the ceo of the cep and a former ambassador to the un under president george w . bush , describes eglyph as a game changer . he believes the algorithm , if adopted widely , could help stem the spread of terrorist propaganda and dissuade radical groups from posting extremist content in the first place . if an extremist group knows that the moment they try to post a video online , that it will be immediately removed and it won t have that viral reach , perhaps it s no longer compelling for them because they can no longer accomplish their propaganda aims , wallace says . lawmakers in europe and the us have called on tech companies to more proactively police terrorist propaganda over the past year , following a spate of high profile attacks . last month , a panel of british lawmakers said in a report that facebook , twitter , and other tech giants are consciously failing to combat isis online , echoing previous statements from leaders in france and germany . no group has been as successful at drawing people into its message than isis , ampnbspmichael steinbach , executive assistant director at the national security branch of the fbi , said in a july ampnbspinterview . ampnbsp far right extremism has become a growing concern , as well . germany has pressured facebook and other companies to more swiftly remove xenophobic content and other material targeting refugees and a study published last week by george washington university s program on extremism found that neo nazis and american white nationalist groups continue to thrive on twitter , even as isis influence has waned on the social network . hany farid , the dartmouth computer science professor who developed photodna and eglyph . dartmouth college tech companies have said they re committed to combatting extremism on their platforms , pointing to statistics on content removal and suspensions . last month , twitter announced that it has suspended , accounts for promoting terrorism since mid , including , accounts since february . the industry appears to be moving toward automation , as well . in a blog post announcing the suspensions , twitter added that it has begun incorporating automated technologies , including proprietary spam fighting tools , to supplement its reporting system . in june , reuters reported that facebook and youtube have quietly begun implementing automated hashing systems , though wallace says that discussions with major social media companies about adopting eglyph are still ongoing . there are a lot of questions around what constitutes extremist or problematic content some experts say that automation could help limit the reach that isis and other groups have on social media , though there are questions over how such technologies would be implemented . unlike child pornography , which is more easily identifiable , extremist content covers a broad spectrum , and laws governing its dissemination vary from country to country . no one would argue that we should allow beheading videos , but there are a lot of questions around what constitutes extremist or problematic content , says john horgan , a professor of psychology at georgia state university who has extensively studied isis and other terrorist groups . some worry that the algorithm would inadvertently block propaganda videos or newsworthy content . although many news outlets have not broadcast isis beheading videos , some have aired parts of the clips that do not show any violence . if those clips bear the same fingerprint as the original video , the algorithm may automatically remove them from facebook and twitter . a lot of news organizations have broadcast portions of those videos , and have made a decision that those excerpts are newsworthy , says vivek krishnamurthy , a clinical instructor at harvard law school s cyberlaw clinic . will those uses that are more innocent also come down automatically ? farid acknowledges that child porn and isis propaganda are two totally different beasts , and that news videos or other non malicious media may be inadvertently swept up by his algorithm . but he says mainstream media outlets could be whitelisted , noting that the technology can be calibrated to target any content that violates a company s terms of service . if facebook came today and said we no longer want photos of kittens on our website , we could do that , farid says . automation always comes with false positives some find that prospect troubling . digital rights groups have long expressed concerns over allowing facebook and twitter to be arbiters of free speech , and they fear that an automated system would lead to more widespread censorship . jillian york , director for international freedom of expression at the electronic frontier foundation , cautioned in an email that automation always comes with false positives , adding that sometimes it s not merely the technology , but also the implementation or implementers , that are flawed . joe mcnamee , executive director of the brussels based advocacy group european digital rights edri , said in an interview that efforts to automate content removal may actually hinder counter speech , which a recent google backed study found to be an effective way of sparking online debate . is it really proportionate to scan filter every single upload from every single european , to make sure it is legal ? edri wrote in a blog post in july . if europe takes the lead in mass surveillance and filtering of their citizens uploads to the internet , what hope is there for the open and democratic internet elsewhere in the world ? the bigger question , perhaps , is whether removing extremist content will have any impact on radicalization . politicians have linked online radicalization to recent terrorist attacks , as president barack obama did ampnbspafter the orlando shooting in june . but horgan says the connection is still fuzzy . most people , i suppose , would rightfully assume that we have some sort of baseline understanding of the role that specific content plays in the radicalization process , he says . well guess what ? we don t . the hard drives of american jihadis , he adds , often contain more innocuous content porn , games , cat videos whereas the more gruesome material is probably more important for the fanboys and supporters than it is for operational members . still , horgan acknowledges that the algorithm could play an important role in reducing the footprint of terrorist propaganda , which he says would be a significant move in a grand strategic and psychological sense . farid expects the cat and mouse game to continue , as well . if major social media companies do adopt his algorithm , he has no doubt that extremist groups will try to work around it . and if they succeed , he says he ll continue to tweak the technology . no matter what technology you develop , the adversary will develop a counter technology , farid says . i don t consider that a problem , i consider that a reality . 